{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 1 : Find-S"
      ],
      "metadata": {
        "id": "j-IS_moWiiIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JgG_VxY0iYpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "0b63a4ba-5f35-46b3-ea20-c9a18d16c05c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file 'enjoysport.csv' created successfully.\n",
            "\n",
            "The given training data set\n",
            "\n",
            "['Weather', 'Temperature', 'Humidity', 'Wind', 'Water', 'Forecast', 'Play']\n",
            "['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'yes']\n",
            "['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'yes']\n",
            "['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'no']\n",
            "['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'yes']\n",
            "\n",
            "The initial value of hypothesis:\n",
            "['0', '0', '0', '0', '0', '0']\n",
            "\n",
            "Find S: Finding a maximally specific hypothesis\n",
            "\n",
            "For training instance no:1 the hypothesis is: ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same']\n",
            "For training instance no:2 the hypothesis is: ['Sunny', 'Warm', '?', 'Strong', 'Warm', 'Same']\n",
            "For training instance no:4 the hypothesis is: ['Sunny', 'Warm', '?', 'Strong', '?', '?']\n",
            "\n",
            "The maximally specific hypothesis for a given training examples:\n",
            "\n",
            "['Sunny', 'Warm', '?', 'Strong', '?', '?']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_963bfca5-bf38-447f-b8a0-90a493718b8a\", \"enjoysport.csv\", 212)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "def create_csv_file(filename):\n",
        "    data = [\n",
        "        ['Weather', 'Temperature', 'Humidity', 'Wind', 'Water', 'Forecast', 'Play'],\n",
        "        ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'yes'],\n",
        "        ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'yes'],\n",
        "        ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'no'],\n",
        "        ['Sunny', 'Warm', 'High', 'Strong', 'Cool', 'Change', 'yes']\n",
        "    ]\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerows(data)\n",
        "        print(f\"CSV file '{filename}' created successfully.\")\n",
        "\n",
        "def find_s_algorithm(filename, num_attributes):\n",
        "    a = []\n",
        "    print(\"\\nThe given training data set\\n\")\n",
        "\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            a.append(row)\n",
        "            print(row)\n",
        "\n",
        "    print(\"\\nThe initial value of hypothesis:\")\n",
        "    hypothesis = ['0'] * num_attributes\n",
        "    print(hypothesis)\n",
        "\n",
        "    for j in range(num_attributes):\n",
        "        hypothesis[j] = a[1][j]\n",
        "\n",
        "    print(\"\\nFind S: Finding a maximally specific hypothesis\\n\")\n",
        "\n",
        "    for i in range(1, len(a)):\n",
        "        if a[i][num_attributes] == 'yes':\n",
        "            for j in range(num_attributes):\n",
        "                if a[i][j] != hypothesis[j]:\n",
        "                    hypothesis[j] = '?'\n",
        "                else:\n",
        "                    hypothesis[j] = a[i][j]\n",
        "            print(f\"For training instance no:{i} the hypothesis is: {hypothesis}\")\n",
        "\n",
        "    print(\"\\nThe maximally specific hypothesis for a given training examples:\\n\")\n",
        "    print(hypothesis)\n",
        "\n",
        "def main():\n",
        "    filename = 'enjoysport.csv'\n",
        "    num_attributes = 6\n",
        "    create_csv_file(filename)\n",
        "    find_s_algorithm(filename, num_attributes)\n",
        "    files.download(filename)\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 2 : Candidate Elimination"
      ],
      "metadata": {
        "id": "bG4NbtWRuzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#step 1: create dataset\n",
        "data = [\n",
        "        ['Sunny', 'Warm', 'Normal', 'Strong', 'Warm', 'Same', 'yes'],\n",
        "        ['Sunny', 'Warm', 'High', 'Strong', 'Warm', 'Same', 'yes'],\n",
        "        ['Rainy', 'Cold', 'High', 'Strong', 'Warm', 'Change', 'no'],\n",
        "        ['Sunny', 'Warm', 'High', 'Low', 'Cool', 'Change', 'yes'],\n",
        "]\n",
        "\n",
        "#step 2: convert the data to the DataFrame\n",
        "columns = ['Outlook','Temperature','Humidity','Wind','Play','Activity','Target']\n",
        "df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "#step 3: save the DataFrame to a CSV fie\n",
        "df.to_csv('dataset.csv',index=False)\n",
        "\n",
        "#step 4: Read the CSV file\n",
        "df = pd.read_csv('dataset.csv')\n",
        "concepts = df.values[:, :-1]\n",
        "target = df.values[:, -1]\n",
        "\n",
        "#step 5: Define the Candidate-Elimination algorithm\n",
        "def learn(concpets,target):\n",
        "  specific_h = concepts[0].copy() #initialize specific hypothesis\n",
        "  general_h = [[\"?\" for _ in range(len(specific_h))] for _ in range(len(specific_h))]\n",
        "\n",
        "  for i, h in enumerate(concepts):\n",
        "    if target[i] == \"yes\":\n",
        "      for x in range(len(specific_h)):\n",
        "        if h[x] != specific_h[x]:\n",
        "          specific_h[x] = '?'\n",
        "          general_h[x][x] = '?'\n",
        "    elif target[i] == \"no\":\n",
        "      for x in range(len(specific_h)):\n",
        "        if h[x] != specific_h[x]:\n",
        "          general_h[x][x] = specific_h[x]\n",
        "        else:\n",
        "          general_h[x][x] = '?'\n",
        "\n",
        "  #Remove all-? hypotheses from general hypotheses\n",
        "  general_h = [gh for gh in general_h if gh != ['?' for _ in range(len(specific_h))]]\n",
        "\n",
        "  return specific_h, general_h\n",
        "\n",
        "#step 6: Run the learning process\n",
        "s_final, g_final = learn(concepts, target)\n",
        "\n",
        "#step 7: print the final specific and general hypotheses\n",
        "print(f\"Final S: {s_final}\")\n",
        "print(f\"Final G: {g_final}\")"
      ],
      "metadata": {
        "id": "btUR70fPu1U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686f38c3-1efb-40a7-9787-ce943366a9ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final S: ['Sunny' 'Warm' '?' '?' '?' '?']\n",
            "Final G: [['Sunny', '?', '?', '?', '?', '?'], ['?', 'Warm', '?', '?', '?', '?']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 3 : ID3"
      ],
      "metadata": {
        "id": "w5rN0xFau1sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from math import log\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "\n",
        "# mount google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# copy path of .csv file\n",
        "df_tennis = pd.read_csv('/content/prog3.csv')\n",
        "\n",
        "def entropy(probs):\n",
        "  return sum([-prob * log(prob, 2) for prob in probs])\n",
        "\n",
        "def entropy_of_list(a_list):\n",
        "  cnt = Counter(x for x in a_list)\n",
        "  num_instances = len(a_list) * 1.0\n",
        "  probs = [x/num_instances for x in cnt.values()]\n",
        "  return entropy(probs)\n",
        "\n",
        "def information_gain(df, split_attribute_name, target_attribute_name):\n",
        "  df_split = df.groupby(split_attribute_name)\n",
        "  nobs = len(df.index) * 1.0\n",
        "  df_agg_ent = df_split.agg({target_attribute_name : [entropy_of_list, lambda x: len(x)/nobs]})[target_attribute_name]\n",
        "  df_agg_ent.columns = ['Entropy','PropObservations']\n",
        "  new_entropy = sum(df_agg_ent['Entropy'] * df_agg_ent['PropObservations'])\n",
        "  old_entropy = entropy_of_list(df[target_attribute_name])\n",
        "  return old_entropy - new_entropy\n",
        "\n",
        "def id3(df, target_attribute_name, attribute_names, default_class=None):\n",
        "  cnt = Counter(x for x in df[target_attribute_name])\n",
        "  if len(cnt) == 1:\n",
        "    return next(iter(cnt))\n",
        "  elif df.empty or (not attribute_names):\n",
        "    return default_class\n",
        "  else:\n",
        "    default_class = max(cnt.keys())\n",
        "    gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names]\n",
        "    index_of_max = gainz.index(max(gainz))\n",
        "    best_attr = attribute_names[index_of_max]\n",
        "    tree = {best_attr: {}}\n",
        "    remaining_attribute_names = [i for i in attribute_names if i!=best_attr]\n",
        "\n",
        "    for attr_val, data_subset in df.groupby(best_attr):\n",
        "      subtree = id3(data_subset, target_attribute_name, remaining_attribute_names)\n",
        "      tree[best_attr][attr_val] = subtree\n",
        "  return tree\n",
        "\n",
        "attribute_names = list(df_tennis.columns)\n",
        "attribute_names.remove('PlayTennis')\n",
        "\n",
        "tree = id3(df_tennis, 'PlayTennis', attribute_names)\n",
        "\n",
        "print(\"\\n\\nThe Resultant Decision Tree is: \\n\")\n",
        "pprint(tree)"
      ],
      "metadata": {
        "id": "Up_e7bfXu6di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
        "\n",
        "\n",
        "The Resultant Decision Tree is:\n",
        "\n",
        "{'Day': {'D1': 'No',\n",
        "         'D10': 'Yes',\n",
        "         'D11': 'Yes',\n",
        "         'D12': 'Yes',\n",
        "         'D13': 'Yes',\n",
        "         'D14': 'No',\n",
        "         'D2': 'No',\n",
        "         'D3': 'Yes',\n",
        "         'D4': 'Yes',\n",
        "         'D5': 'Yes',\n",
        "         'D6': 'No',\n",
        "         'D7': 'Yes',\n",
        "         'D8': 'No',\n",
        "         'D9': 'Yes'}}"
      ],
      "metadata": {
        "id": "HIyx5acoCJnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 4 : ANN"
      ],
      "metadata": {
        "id": "JMO2bZw0uuUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#1.\n",
        "X = np.array(([2, 9], [1, 5], [3, 6]), dtype=float)\n",
        "y = np.array(([92], [86], [89]), dtype=float)\n",
        "X = X/np.amax(X,axis=0) #normalize input data\n",
        "y = y/100 #normalize output data!\n",
        "\n",
        "#2.Activation functions\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "def derivatives_sigmoid(x):\n",
        "  return x * (1 - x)\n",
        "\n",
        "#3.Network structure and Hyperparameters => learning phase\n",
        "epoch=1000\n",
        "learning_rate = 0.6\n",
        "inputlayer_neurons = 2\n",
        "hiddenlayer_neurons = 3\n",
        "output_neurons = 1\n",
        "\n",
        "#4.Weights initializaion (Weights->{wh,wo}, Biases->{bh,bo})\n",
        "wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))\n",
        "bh=np.random.uniform(size=(1,hiddenlayer_neurons))\n",
        "wo=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))\n",
        "bo=np.random.uniform(size=(1,output_neurons))\n",
        "\n",
        "#5.Training the Network\n",
        "for i in range(epoch):\n",
        "  #Forward Propagation\n",
        "  net_h = np.dot(X,wh) + bh\n",
        "  sigma_h = sigmoid(net_h)\n",
        "  net_o = np.dot(sigma_h,wo) + bo\n",
        "  output = sigmoid(net_o)\n",
        "  #Backpropagation =>(to reduce errors)\n",
        "  deltaK = (y-output)*derivatives_sigmoid(output)\n",
        "  deltaH = deltaK.dot(wo.T)*derivatives_sigmoid(sigma_h)\n",
        "  wo = wo + sigma_h.T.dot(deltaK) * learning_rate\n",
        "  wh = wh + X.T.dot(deltaH) * learning_rate\n",
        "print(f\"Input: \\n {X}\")\n",
        "print(f\"Actual Output: \\n{y}\")\n",
        "print(f\"Predicted Output: \\n{output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TygycqIYuw8K",
        "outputId": "9ae78262-d828-497c-d18a-e894875db42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " [[0.66666667 1.        ]\n",
            " [0.33333333 0.55555556]\n",
            " [1.         0.66666667]]\n",
            "Actual Output: \n",
            "[[0.92]\n",
            " [0.86]\n",
            " [0.89]]\n",
            "Predicted Output: \n",
            "[[0.89397188]\n",
            " [0.88468015]\n",
            " [0.89158982]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 5 : Naive Bayes Text Classifier"
      ],
      "metadata": {
        "id": "zQJcs8Lvk04x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "  'text': [\n",
        "    'I love programming in Python',\n",
        "    'Python is an amazing language',\n",
        "    'I hate getting errors in my code',\n",
        "    'Debugging can be frustrating',\n",
        "    'Machine learning is fascinating',\n",
        "    'I dislike syntax errors'],\n",
        "    'label': ['positive', 'positive', 'negative', 'negative', 'positive', 'negative']\n",
        "}\n",
        "\n",
        "#Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Convert text labels to numerical values\n",
        "df['label'] = df['label'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Split dataset into training and test sets with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split( df['text'], df['label'], test_size=0.2, stratify=df['label'], random_state=42 )\n",
        "\n",
        "# Convert text data into feature vectors\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "#Train Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "#Make predictions\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "\n",
        "#Evaluate the model\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of the classifier:\", accuracy)\n",
        "\n",
        "#Predict a single text\n",
        "sample_text = [\"I enjoy learning about artificial intelligence\"]\n",
        "sample_vec = vectorizer.transform(sample_text)\n",
        "predicted_label = clf.predict(sample_vec)\n",
        "print(\"Predicted label for sample text:\", \"positive\" if predicted_label[0] == 1 else \"negative\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNrIpWuZk2lA",
        "outputId": "1b3a7dfe-aa7b-48e5-bed1-89abf8c21ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the classifier: 1.0\n",
            "Predicted label for sample text: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 6 :"
      ],
      "metadata": {
        "id": "BP2641zBnG2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probAttr(data, attr, val):\n",
        "    Total = data.shape[0]\n",
        "    cnt = len(data[data[attr]==val])\n",
        "    return cnt, cnt/Total\n",
        "\n",
        "def train(data, Attr, conceptVals, concept):\n",
        "    conceptProbs = {}\n",
        "    countConcept = {}\n",
        "    for cVal in conceptVals:\n",
        "      countConcept[cVal], conceptProbs[cVal] = probAttr(data, concept, cVal)\n",
        "    AttrConcept = {}\n",
        "    probability_list = {}\n",
        "    for att in Attr:\n",
        "      probability_list[att] = {}\n",
        "      AttrConcept[att] = {}\n",
        "      for val in Attr[att]:\n",
        "        AttrConcept[att][val] = {}\n",
        "        a, probability_list[att][val] = probAttr(data, att, val)\n",
        "        for cVal in conceptVals:\n",
        "          dataTemp = data[data[att]==val]\n",
        "          AttrConcept[att][val][cVal] = len(dataTemp[dataTemp[concept]==cVal])/countConcept[cVal]\n",
        "    print(f\"P(A) : {conceptProbs}\\n\") #Prior Probabilities of each concept\n",
        "    print(f\"P(X/A) : {AttrConcept}\\n\") #Attribute probability for each attr\n",
        "    print(f\"P(X) : {probability_list}\\n\") #conditional probability?\n",
        "    return conceptProbs, AttrConcept, probability_list\n",
        "\n",
        "def test(examples, Attr, concept_list, conceptProbs, AttrConcept, probability_list):\n",
        "    misclassification_count = 0\n",
        "    Total = len(examples)\n",
        "    for ex in examples:\n",
        "      px = {}\n",
        "      for a in Attr:\n",
        "        for x in ex:\n",
        "          for c in concept_list:\n",
        "            if x in AttrConcept[a]:\n",
        "              if c not in px:\n",
        "                px[c] = conceptProbs[c] * AttrConcept[a][x][c] / probability_list[a][x]\n",
        "              else:\n",
        "                px[c] = px[c] * AttrConcept[a][x][c] / probability_list[a][x]\n",
        "      print(px)\n",
        "      classification = max(px, key=px.get)\n",
        "      print(f\"Classification: {classification} Expected: {ex[-1]}\")\n",
        "      if (classification != ex[-1]):\n",
        "        misclassification_count += 1\n",
        "        misclassification_rate = misclassification_count*100/Total\n",
        "        accuracy = 100 - misclassification_rate\n",
        "    print(f\"Misclassification Count: {misclassification_count}\")\n",
        "    print(f\"Misclassification Rate: {misclassification_rate}\")\n",
        "    print(f\"Accuracy = {accuracy}%\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/home/sahyadri/Desktop/ML/tennis.csv')\n",
        "concept = str(list(df)[-1])\n",
        "concept_list = set(df[concept])\n",
        "Attr = {}\n",
        "for a in df.columns[:-1]:\n",
        "  Attr[a] = set(df[a])\n",
        "  print(f\"{a}  :{Attr[a]}\")\n",
        "conceptProbs, AttrConcept, probability_list = train(df, Attr, concept_list, concept)\n",
        "examples = pd.read_csv('/home/sahyadri/Desktop/ML/tennis.csv')\n",
        "test(examples.values, Attr, concept_list, conceptProbs, AttrConcept, probability_list)"
      ],
      "metadata": {
        "id": "TE3T3xnknLM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "outlook: {'sunny', 'overcast', 'rain'}\n",
        "temperature: {'cool', 'mild', 'hot'}\n",
        "humidity: {'normal', 'high'}\n",
        "wind: {'strong', 'weak'}\n",
        "P(A) : {'no': 0.35714285714285715, 'yes': 0.6428571428571429}\n",
        "\n",
        "P(X/A) : {'outlook': {'sunny': {'no': 0.6, 'yes': 0.2222222222222222}, 'overcast': {'no': 0.0, 'yes': 0.4444444444444444}, 'rain': {'no': 0.4, 'yes': 0.3333333333333333}},\n",
        "          'temperature': {'cool': {'no': 0.2, 'yes': 0.3333333333333333}, 'mild': {'no': 0.4, 'yes': 0.4444444444444444}, 'hot': {'no': 0.4, 'yes': 0.2222222222222222}},\n",
        "          'humidity': {'normal': {'no': 0.2, 'yes': 0.6666666666666666}, 'high': {'no': 0.8, 'yes': 0.3333333333333333}},\n",
        "          'wind': {'strong': {'no': 0.6, 'yes': 0.3333333333333333}, 'weak': {'no': 0.4, 'yes': 0.6666666666666666}}}\n",
        "\n",
        "P(X) : {'outlook': {'sunny': 0.35714285714285715, 'overcast': 0.2857142857142857, 'rain': 0.35714285714285715},\n",
        "        'temperature': {'cool': 0.2857142857142857, 'mild': 0.42857142857142855, 'hot': 0.2857142857142857},\n",
        "        'humidity': {'normal': 0.5, 'high': 0.5},\n",
        "        'wind': {'strong': 0.42857142857142855, 'weak': 0.5714285714285714}}\n",
        "\n",
        "{'no': 0.9408000000000002, 'yes': 0.2419753086419753}\n",
        "Classification : no Expected : no\n",
        "{'no': 1.8816000000000002, 'yes': 0.16131687242798354}\n",
        "Classification : no Expected : no\n",
        "{'no': 0.0, 'yes': 0.6049382716049383}\n",
        "Classification : yes Expected : yes\n",
        "\n",
        "{'no': 0.4181333333333335, 'yes': 0.4839506172839506}\n",
        "Classification : yes Expected : yes\n",
        "{'no': 0.07840000000000004, 'yes': 1.0888888888888888}\n",
        "Classification : yes Expected : yes\n",
        "\n",
        "{'no': 0.15680000000000005, 'yes': 0.7259259259259259}\n",
        "Classification : yes Expected : no\n",
        "{'no': 0.0, 'yes': 1.2098765432098766}\n",
        "Classification : yes Expected : yes\n",
        "\n",
        "\n",
        "{'no': 0.6272000000000001, 'yes': 0.3226337448559671}\n",
        "Classification : no Expected : no\n",
        "{'no': 0.11760000000000002, 'yes': 0.7259259259259256}\n",
        "Classification : yes Expected : yes\n",
        "{'no': 0.10453333333333338, 'yes': 0.9679012345679012}\n",
        "Classification : yes Expected : yes\n",
        "\n",
        "{'no': 0.31360000000000005, 'yes': 0.43017832647462273}\n",
        "Classification : yes Expected : yes\n",
        "{'no': 0.0, 'yes': 0.5377229080932785}\n",
        "Classification : yes Expected : yes\n",
        "{'no': 0.0, 'yes': 1.2098765432098766}\n",
        "Classification : yes Expected : yes\n",
        "\n",
        "{'no': 0.8362666666666669, 'yes': 0.3226337448559671}\n",
        "Classification : no Expected : no\n",
        "Misclassification Count=1\n",
        "Misclassification Rate=7.142857142857143%\n",
        "Accuracy=92.85714285714286%"
      ],
      "metadata": {
        "id": "aYin2OfynhqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 7 : (Jupyter Notebook) NB"
      ],
      "metadata": {
        "id": "InvAmjz0nsCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv(\"/home/sahyadri/Downloads/Heart_Disease (1).csv\")\n",
        "\n",
        "feature_col_names = df.columns[df.columns != 'CHDRisk']\n",
        "predicted_class_names=['CHDRisk']\n",
        "\n",
        "X = df[feature_col_names]\n",
        "y=df[predicted_class_names]\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "categorical_columns = X.select_dtypes(include = [object]).columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    X[col] = label_encoder.fit_transform(X[col])\n",
        "\n",
        "y=label_encoder.fit_transform(y)\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)\n",
        "print('Total number of Training Data:',y_train.shape)\n",
        "print('Total number of Test Data:',y_test.shape)\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train,y_train)\n",
        "predicted = clf.predict(X_test)\n",
        "\n",
        "accuracy = metrics.accuracy_score(y_test,predicted)\n",
        "print('\\n Accuracy of the cassifier:',accuracy)\n",
        "\n",
        "conf_matrix = metrics.confusion_matrix(y_test,predicted)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf_matrix,annot=True,fmt='d',cmap='Blues',xticklabels=['No disease','Heart Disease'],yticklabels=['No Disease','Heart Disease'])\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "test_data=[[0,63,2,0,5,0,0,0,0,240,120,80,23.5,70,88]]\n",
        "predict_test_data = clf.predict(test_data)\n",
        "print('\\n Predicted value for individual test data',predict_test_data)"
      ],
      "metadata": {
        "id": "JAjWgTrZAXQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROGRAM 8: KNN"
      ],
      "metadata": {
        "id": "qYbJlXUDAXl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the required packages\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import datasets\n",
        "\n",
        "#Lood dataset\n",
        "iris = datasets.load_iris()\n",
        "print(\"Iris Data set loaded...\")\n",
        "\n",
        "#Split the data into train and test samples\n",
        "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1)\n",
        "print(\"Dataset is split into training and testing...\")\n",
        "print(\"Size of training data and its label\", x_train.shape, y_train.shape)\n",
        "print(\"Size of testing data and its label\", x_test.shape, y_test.shape)\n",
        "\n",
        "#Print Label numbers and their names\n",
        "for i in range(len(iris.target_names)):\n",
        "  print(\"Label\", i, \"-\", str(iris.target_names[i]))\n",
        "\n",
        "#Create an object of KNN classifier\n",
        "classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "#Perform Training\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "#Perform testing\n",
        "y_pred = classifier.predict(x_test)\n",
        "\n",
        "#Display the results\n",
        "print(\"Results of Classification using K-nn with K-1\")\n",
        "for r in range(0, len(x_test)):\n",
        "  print(\"Sample:\", str(x_test[r]), \"Actual-label:\", str(y_test[r]), \"Predicted-label:\", str(y_pred[r]))\n",
        "\n",
        "#Print Classification Accuracy\n",
        "print(\"Classification Accuracy:\", classifier.score(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GzUJo0GAYC_",
        "outputId": "93b38b4b-c4f1-4c98-a760-71273097d9ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Data set loaded...\n",
            "Dataset is split into training and testing...\n",
            "Size of training data and its label (135, 4) (135,)\n",
            "Size of testing data and its label (15, 4) (15,)\n",
            "Label 0 - setosa\n",
            "Label 1 - versicolor\n",
            "Label 2 - virginica\n",
            "Results of Classification using K-nn with K-1\n",
            "Sample: [6.4 3.1 5.5 1.8] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [5.2 3.5 1.5 0.2] Actual-label: 0 Predicted-label: 0\n",
            "Sample: [5.8 2.7 3.9 1.2] Actual-label: 1 Predicted-label: 1\n",
            "Sample: [7.  3.2 4.7 1.4] Actual-label: 1 Predicted-label: 1\n",
            "Sample: [5.  3.5 1.6 0.6] Actual-label: 0 Predicted-label: 0\n",
            "Sample: [6.7 3.3 5.7 2.5] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [4.7 3.2 1.3 0.2] Actual-label: 0 Predicted-label: 0\n",
            "Sample: [7.7 2.6 6.9 2.3] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [6.5 3.  5.5 1.8] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [4.6 3.1 1.5 0.2] Actual-label: 0 Predicted-label: 0\n",
            "Sample: [5.8 2.7 5.1 1.9] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [6.7 3.1 4.7 1.5] Actual-label: 1 Predicted-label: 1\n",
            "Sample: [6.7 2.5 5.8 1.8] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [7.9 3.8 6.4 2. ] Actual-label: 2 Predicted-label: 2\n",
            "Sample: [7.3 2.9 6.3 1.8] Actual-label: 2 Predicted-label: 2\n",
            "Classification Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}